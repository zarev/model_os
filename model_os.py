"""model_os module"""

# uvicorn model_os:app --reload
# pylint: disable=fixme

from io import BytesIO
import queue
import threading
from contextlib import asynccontextmanager
from typing import Optional
import base64
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from PIL import Image
from pydub import AudioSegment
from torch import float16
from transformers import AutoModelForCausalLM, AutoProcessor, AutoModelForSpeechSeq2Seq
from numpy import float32, int16, array, frombuffer
from pydub import AudioSegment


# import debugpy
# debugpy.listen(("0.0.0.0", 5678))
# print("Waiting for debugger to attach...")
# debugpy.wait_for_client()

@asynccontextmanager
async def lifespan(my_app: FastAPI):  # pylint: disable=unused-argument
    """Manages lifespan of app"""
    # Application startup
    # load models
    load(LoadRequest(num_gpus=1, models_per_gpu=1, model_name="phi"))
    load(LoadRequest(num_gpus=1, models_per_gpu=1, model_name="whisper"))
    yield
    # Application shutdown
    models.clear()

app = FastAPI(lifespan=lifespan)

class Model:
    """
    A class representing an inference model.

    Attributes:
    -----------
    model_path : str
        The file path to the model.
    index : ints
        The index identifying the model instance.
    loaded : bool
        A flag indicating whether the model is currently loaded.

    Methods:
    --------
    load():
        Loads the model and initializes it.

    run(input: tuple[Optional[str], str], output: queue.Queue) -> None:
        Processes input text and image using the loaded model and generates a response.

    prompt(image: Optional[str], text: str) -> str:
        Runs the `run` method in a separate thread and retrieves the result.
    """

    def __init__(self, model_path: str, index: int):
        """
        Initializes the Model class with a given model path and index.

        Parameters:

        model_path : str
            The file path to the model.
        index : int
            The index identifying the model instance.
        """
        self.model = None
        self.model_path = model_path
        self.model_type = AutoModelForCausalLM
        self.use_safetensors = None
        self.model_name = "Phi-3-vision-128k-instruct"
        if model_path.count('whisper') == 1:
            self.model_type = AutoModelForSpeechSeq2Seq
            self.use_safetensors = True
            self.model_name = "whisper"
        self.index = index
        self.loaded = False
        self.processor = None
        self.tokenizer_stream = None

    def load(self):
        """Loads model components"""
        # tensorflow model loading logic
        self.model = self.model_type.from_pretrained(
            self.model_name,
            # app root dir
            cache_dir=self.model_path - self.model_name,
            use_safetensors=self.use_safetensors,
            device_map="cuda",
            trust_remote_code=True,
            torch_dtype="auto",
            # Use _attn_implementation='eager' to disable flash attention, or 'flash_attention_2'
            _attn_implementation="eager")

        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.loaded = True

    def run(self, input_data: tuple[Optional[str], Optional[str], str], output: queue.Queue) -> None:
        """
        Processes input text, image, and audio using the loaded model and generates a response.
        
        -------
        ### Parameters:
        input:
            - input_image (Optional[str]): Base64-encoded image string, or None.
            - input_audio (Optional[str]): Base64-encoded audio string, or None.
            - input_text (str): The text prompt to be processed.

        output : queue.Queue
             - A queue used to store the generated response or an error message for retrieval by the calling thread.

        -------
        ### Description:

        This method processes the provided input text and optionally an image and audio input using the loaded model.
        If an audio input is provided, it is processed to extract features, which are then transcribed and appended
        to the input text. If an image input is provided, it is decoded and incorporated into the text prompt.
        The method handles exceptions during processing and places error messages into the output queue.
        The response from the model is generated by decoding tokens incrementally and is placed into the output queue
        when processing is complete.

        ------
        ### Notes:
        
        - The input audio, if provided, is processed using a speech recognition model (e.g., Whisper), and the transcription
        is appended to the input text.
        - The input image, if provided, is processed and incorporated into the prompt for the model.
        - The method uses CUDA for processing if available, and handles exceptions gracefully by placing appropriate
        error messages in the output queue.
        """
        # TODO add file input option
        input_image, input_audio, input_text = input_data

        if input_audio:

            audio_samples = process_audio(input_audio)
            input_features = models['whisper'].processor(audio_samples, 
                                                            sampling_rate=16000, 
                                                            return_tensors="pt", 
                                                            return_attention_mask=True)

            input_features = input_features.to("cuda", dtype=float16)

            # Model inference with attention mask
            pred_ids = models["whisper"].model.generate(input_features.input_features, max_new_tokens=500)
            # Decode the predicted IDs to get the transcription
            pred_text = models['whisper'].processor.batch_decode(pred_ids, skip_special_tokens=True)[0]

            # Append the transcription to the input text
            input_text += pred_text
    
        messages = [{
            "role": "user",
            "content": ''}]

        image_obj = None
        # If there is image load it and add tag to prompt
        if input_image:
            image_data = base64.b64decode(input_image)
            image_obj = Image.open(BytesIO(image_data))
            messages[0]["content"] += "<|image_1|>"

        messages[0]["content"] += f"{input_text}"

        templated_prompt = self.processor.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True)

        try:
            inputs = self.processor(templated_prompt, image_obj, return_tensors="pt").to("cuda:0")

        except RuntimeError as e:
            # for some reason the error only goes to output
            # if there is a print statement first, otherwise
            # output is empty on client.
            # TODO investigate why this is
            print(flush=True)
            output.put(f"Error occurred during input processing: {e}")
            return

        generation_args = {
		"max_new_tokens": 2048,
		"temperature": 0.1,
		"do_sample": True,}

        generate_ids = self.model.generate(
            **inputs,
            eos_token_id=self.processor.tokenizer.eos_token_id,
            **generation_args)

        # Remove input tokens
        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]

        response = self.processor.batch_decode(
            generate_ids,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False)[0]

        output.put(response)
        output.put(None)

    def generate(self, image: Optional[str], audio: Optional[str], text: str) -> str:
        """
        Runs the `run` method in a separate thread and retrieves the result.
        
        -----
        ### Parameters:
        
        image (str): Path to the image, or None if not provided.
        text (str): The text prompt for processing.
        audio (str): Optional audio file input

        -----
        ### Returns:
        str: The processing result or an error message.
        """
        output = queue.Queue()
        thread = threading.Thread(
            target=self.run, args=((image, audio, text), output))
        thread.daemon = True
        thread.start()

        result = []
        while thread.is_alive():
            try:
                text = output.get()
                if text is None:
                    output.put(
                        'Error: Empty response')
                    break
                result.append(text)
            except queue.Empty:
                output.put('''Error: No response received from the thread.
                           The processing might be taking longer than expected.
                           Consider checking the model, inputs, and system resources.''')
                break

        thread.join()
        return "".join(result)

def process_audio(input_audio: str) -> array: #np.array
            """
            Decodes a base64-encoded audio string, converts it to an audio segment,
            resamples it to 16 kHz, and returns the audio samples as a NumPy array.

            Args:
                input_audio (str): A base64-encoded string representing the audio data.

            Returns:
                numpy.ndarray: An array of audio samples.
            """
            b64_bytes = base64.b64decode(input_audio)
            arr_contents = frombuffer(b64_bytes, dtype=float32)
            audio_array_int16 = (arr_contents * 32768).astype(int16)
            audio_segment = AudioSegment(data=audio_array_int16.tobytes(), sample_width=2, frame_rate=44100, channels=1)
            audio_segment = audio_segment.set_frame_rate(16000)
            audio_samples = array(audio_segment.get_array_of_samples())

            return audio_samples

# Store loaded models in a dictionary
models = {}


class LoadRequest(BaseModel):
    '''Initial load request on start with default values'''
    num_gpus: int = 1
    models_per_gpu: int = 1
    model_name: str = "phi"


@app.post("/load")
def load(request: LoadRequest) -> dict[str, str]:
    '''
    Loads specified models into memory during application startup.
    '''

    root_path = "/app/models/"
    for gpu_index in range(request.num_gpus):
        for _ in range(request.models_per_gpu):
            # TODO: investigate how to stop shard loading
            phi = Model(root_path.join("Phi-3-vision-128k-instruct"), gpu_index)
            phi.load()

            whisper = Model(root_path.join("distil-large-v3"), gpu_index)
            phi.load()

            models[request.model_name] = phi
            models[request.model_name] = whisper

    return {"status": "200 OK", "model_name": request.model_name}


class PromptRequest(BaseModel):
    '''Definition of the client request'''
    model: int = 0
    image: Optional[str] = None
    audio: Optional[str] = None
    text: str
    model_name: str = "phi"


@app.post("/prompt")
def prompt(request: PromptRequest):
    """
    Processes a text prompt and optional image and optional audio.

    This function retrieves the specified model, processes the input,
    and returns the model's generated response.
    """

    if request.model_name not in models:
        raise HTTPException(
            status_code=404, detail="Model not found but tried to prompt.")

    model = models[request.model_name]

    request_prompt = request.text

    audio = None
    if request.audio is not None:
        audio = request.audio

    image = None
    if request.image is not None:
        image = request.image

    response_text = model.generate(image=image, text=request_prompt, audio=audio)

    return {"response": response_text}
